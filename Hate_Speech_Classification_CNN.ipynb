{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Input, Embedding, Activation, Flatten, Dense\n",
    "from keras.layers import Conv1D, MaxPooling1D, Dropout\n",
    "from keras.models import Model\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>hate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>explanation why the edits made under my userna...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>d'aww! he matches this background colour i'm s...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>hey man, i'm really not trying to edit war. it...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>\" more i can't make any real suggestions on im...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>you, sir, are my hero. any chance you remember...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>\"  congratulations from me as well, use the to...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>cocksucker before you piss around on my work</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>your vandalism to the matt shirvington article...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>sorry if the word 'nonsense' was offensive to ...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>alignment on this subject and which are contra...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>10</td>\n",
       "      <td>\" fair use rationale for image:wonju.jpg  than...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>11</td>\n",
       "      <td>bbq   be a man and lets discuss it-maybe over ...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>12</td>\n",
       "      <td>hey... what is it.. @ | talk . what is it... a...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>13</td>\n",
       "      <td>before you start throwing accusations and warn...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>14</td>\n",
       "      <td>oh, and the girl above started her arguments w...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>15</td>\n",
       "      <td>\"  juelz santanas age  in 2002, juelz santana ...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>16</td>\n",
       "      <td>bye!   don't look, come or think of comming ba...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>17</td>\n",
       "      <td>redirect talk:voydan pop georgiev- chernodrinski</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>18</td>\n",
       "      <td>the mitsurugi point made no sense - why not ar...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>19</td>\n",
       "      <td>don't mean to bother you   i see that you're w...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>20</td>\n",
       "      <td>\"   regarding your recent edits   once again, ...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>21</td>\n",
       "      <td>\" good to know. about me, yeah, i'm studying n...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>22</td>\n",
       "      <td>\"   snowflakes are not always symmetrical!   u...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>23</td>\n",
       "      <td>\"   the signpost: 24 september 2012    read th...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>24</td>\n",
       "      <td>\"  re-considering 1st paragraph edit? i don't ...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>25</td>\n",
       "      <td>radial symmetry   several now extinct lineages...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>26</td>\n",
       "      <td>there's no need to apologize. a wikipedia arti...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>27</td>\n",
       "      <td>yes, because the mother of the child in the ca...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>28</td>\n",
       "      <td>\" ok. but it will take a bit of work but i can...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>29</td>\n",
       "      <td>\"== a barnstar for you! ==    the real life ba...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202206</th>\n",
       "      <td>202206</td>\n",
       "      <td>thanks gemma</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202207</th>\n",
       "      <td>202207</td>\n",
       "      <td>judd is a  &amp;amp; #homophobic #freemilo #milo #...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202208</th>\n",
       "      <td>202208</td>\n",
       "      <td>lady banned from kentucky mall.  #jcpenny #ken...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202209</th>\n",
       "      <td>202209</td>\n",
       "      <td>ugh i'm trying to enjoy my happy hour drink &amp;a...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202210</th>\n",
       "      <td>202210</td>\n",
       "      <td>want to know how to live a   life? do more thi...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202211</th>\n",
       "      <td>202211</td>\n",
       "      <td>love island รฐ???</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202212</th>\n",
       "      <td>202212</td>\n",
       "      <td>my fav actor #vijaysethupathi ! my fav actress...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202213</th>\n",
       "      <td>202213</td>\n",
       "      <td>whew  รฐ??? it's a productive and   #friday!!!</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202214</th>\n",
       "      <td>202214</td>\n",
       "      <td>she's finally here!</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202215</th>\n",
       "      <td>202215</td>\n",
       "      <td>passed first year of uni #yay #love #pass #uni...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202216</th>\n",
       "      <td>202216</td>\n",
       "      <td>this week is flying by   #humpday - #wednesday...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202217</th>\n",
       "      <td>202217</td>\n",
       "      <td>modeling photoshoot this friday yay #model #m...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202218</th>\n",
       "      <td>202218</td>\n",
       "      <td>you're surrounded by people who love you (even...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202219</th>\n",
       "      <td>202219</td>\n",
       "      <td>feel like... รฐ???รฐ??ยถรฐ??? #dog #summer #hot #h...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202220</th>\n",
       "      <td>202220</td>\n",
       "      <td>omfg i'm offended! i'm a  mailbox and i'm prou...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202221</th>\n",
       "      <td>202221</td>\n",
       "      <td>you don't have the balls to hashtag me as a  b...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202222</th>\n",
       "      <td>202222</td>\n",
       "      <td>makes you ask yourself, who am i? then am i a...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202223</th>\n",
       "      <td>202223</td>\n",
       "      <td>hear one of my new songs! don't go - katie ell...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202224</th>\n",
       "      <td>202224</td>\n",
       "      <td>you can try to 'tail' us to stop, 'butt' we'r...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202225</th>\n",
       "      <td>202225</td>\n",
       "      <td>i've just posted a new blog: #secondlife #lone...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202226</th>\n",
       "      <td>202226</td>\n",
       "      <td>you went too far with</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202227</th>\n",
       "      <td>202227</td>\n",
       "      <td>good morning #instagram #shower #water #berlin...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202228</th>\n",
       "      <td>202228</td>\n",
       "      <td>#holiday   bull up: you will dominate your bul...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202229</th>\n",
       "      <td>202229</td>\n",
       "      <td>less than 2 weeks รฐ???รฐ???รฐ??ยผรฐ??ยนรฐ???รฐ??ยต #ib...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202230</th>\n",
       "      <td>202230</td>\n",
       "      <td>off fishing tomorrow carnt wait first time in ...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202231</th>\n",
       "      <td>202231</td>\n",
       "      <td>ate isz that youuu?รฐ???รฐ???รฐ???รฐ???รฐ???รฐ???รฐ??...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202232</th>\n",
       "      <td>202232</td>\n",
       "      <td>to see nina turner on the airwaves trying to...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202233</th>\n",
       "      <td>202233</td>\n",
       "      <td>listening to sad songs on a monday morning otw...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202234</th>\n",
       "      <td>202234</td>\n",
       "      <td>#sikh #temple vandalised in in #calgary, #wso ...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202235</th>\n",
       "      <td>202235</td>\n",
       "      <td>thank you for you follow</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>202236 rows ร 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Unnamed: 0                                       comment_text  hate\n",
       "0                0  explanation why the edits made under my userna...   0.0\n",
       "1                1  d'aww! he matches this background colour i'm s...   0.0\n",
       "2                2  hey man, i'm really not trying to edit war. it...   0.0\n",
       "3                3  \" more i can't make any real suggestions on im...   0.0\n",
       "4                4  you, sir, are my hero. any chance you remember...   0.0\n",
       "5                5  \"  congratulations from me as well, use the to...   0.0\n",
       "6                6       cocksucker before you piss around on my work   1.0\n",
       "7                7  your vandalism to the matt shirvington article...   0.0\n",
       "8                8  sorry if the word 'nonsense' was offensive to ...   0.0\n",
       "9                9  alignment on this subject and which are contra...   0.0\n",
       "10              10  \" fair use rationale for image:wonju.jpg  than...   0.0\n",
       "11              11  bbq   be a man and lets discuss it-maybe over ...   0.0\n",
       "12              12  hey... what is it.. @ | talk . what is it... a...   1.0\n",
       "13              13  before you start throwing accusations and warn...   0.0\n",
       "14              14  oh, and the girl above started her arguments w...   0.0\n",
       "15              15  \"  juelz santanas age  in 2002, juelz santana ...   0.0\n",
       "16              16  bye!   don't look, come or think of comming ba...   1.0\n",
       "17              17   redirect talk:voydan pop georgiev- chernodrinski   0.0\n",
       "18              18  the mitsurugi point made no sense - why not ar...   0.0\n",
       "19              19  don't mean to bother you   i see that you're w...   0.0\n",
       "20              20  \"   regarding your recent edits   once again, ...   0.0\n",
       "21              21  \" good to know. about me, yeah, i'm studying n...   0.0\n",
       "22              22  \"   snowflakes are not always symmetrical!   u...   0.0\n",
       "23              23  \"   the signpost: 24 september 2012    read th...   0.0\n",
       "24              24  \"  re-considering 1st paragraph edit? i don't ...   0.0\n",
       "25              25  radial symmetry   several now extinct lineages...   0.0\n",
       "26              26  there's no need to apologize. a wikipedia arti...   0.0\n",
       "27              27  yes, because the mother of the child in the ca...   0.0\n",
       "28              28  \" ok. but it will take a bit of work but i can...   0.0\n",
       "29              29  \"== a barnstar for you! ==    the real life ba...   0.0\n",
       "...            ...                                                ...   ...\n",
       "202206      202206                                     thanks gemma     0.0\n",
       "202207      202207  judd is a  &amp; #homophobic #freemilo #milo #...   1.0\n",
       "202208      202208  lady banned from kentucky mall.  #jcpenny #ken...   1.0\n",
       "202209      202209  ugh i'm trying to enjoy my happy hour drink &a...   0.0\n",
       "202210      202210  want to know how to live a   life? do more thi...   0.0\n",
       "202211      202211                                 love island รฐ???     0.0\n",
       "202212      202212  my fav actor #vijaysethupathi ! my fav actress...   0.0\n",
       "202213      202213      whew  รฐ??? it's a productive and   #friday!!!   0.0\n",
       "202214      202214                             she's finally here!      0.0\n",
       "202215      202215  passed first year of uni #yay #love #pass #uni...   0.0\n",
       "202216      202216  this week is flying by   #humpday - #wednesday...   0.0\n",
       "202217      202217   modeling photoshoot this friday yay #model #m...   0.0\n",
       "202218      202218  you're surrounded by people who love you (even...   0.0\n",
       "202219      202219  feel like... รฐ???รฐ??ยถรฐ??? #dog #summer #hot #h...   0.0\n",
       "202220      202220  omfg i'm offended! i'm a  mailbox and i'm prou...   1.0\n",
       "202221      202221  you don't have the balls to hashtag me as a  b...   1.0\n",
       "202222      202222   makes you ask yourself, who am i? then am i a...   1.0\n",
       "202223      202223  hear one of my new songs! don't go - katie ell...   0.0\n",
       "202224      202224   you can try to 'tail' us to stop, 'butt' we'r...   0.0\n",
       "202225      202225  i've just posted a new blog: #secondlife #lone...   0.0\n",
       "202226      202226                            you went too far with     0.0\n",
       "202227      202227  good morning #instagram #shower #water #berlin...   0.0\n",
       "202228      202228  #holiday   bull up: you will dominate your bul...   0.0\n",
       "202229      202229  less than 2 weeks รฐ???รฐ???รฐ??ยผรฐ??ยนรฐ???รฐ??ยต #ib...   0.0\n",
       "202230      202230  off fishing tomorrow carnt wait first time in ...   0.0\n",
       "202231      202231  ate isz that youuu?รฐ???รฐ???รฐ???รฐ???รฐ???รฐ???รฐ??...   0.0\n",
       "202232      202232    to see nina turner on the airwaves trying to...   0.0\n",
       "202233      202233  listening to sad songs on a monday morning otw...   0.0\n",
       "202234      202234  #sikh #temple vandalised in in #calgary, #wso ...   1.0\n",
       "202235      202235                         thank you for you follow     0.0\n",
       "\n",
       "[202236 rows x 3 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"hate_speech_lower_data.csv\", encoding='ISO-8859-1')\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data['comment_text'] = data['comment_text'].str.replace(r'[0-9]+', '')\n",
    "\n",
    "X = data['comment_text'].values\n",
    "y = data['hate'].values\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf = Tokenizer(num_words=10000, char_level=False, oov_token='UNK')\n",
    "tf.fit_on_texts(X)\n",
    "\n",
    "train_sequences = tf.texts_to_sequences(X_train)\n",
    "test_sequences = tf.texts_to_sequences(X_test)\n",
    "\n",
    "# Padding\n",
    "train_data = pad_sequences(train_sequences, maxlen=500, padding='post')\n",
    "test_data = pad_sequences(test_sequences, maxlen=500, padding='post')\n",
    "\n",
    "\n",
    "\n",
    "# Convert to numpy array\n",
    "train_data = np.array(train_data, dtype='float32')\n",
    "test_data = np.array(test_data, dtype='float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_4 (Embedding)      (None, 500, 32)           320000    \n",
      "_________________________________________________________________\n",
      "conv1d_7 (Conv1D)            (None, 499, 16)           1040      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_7 (MaxPooling1 (None, 249, 16)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_8 (Conv1D)            (None, 248, 32)           1056      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_8 (MaxPooling1 (None, 124, 32)           0         \n",
      "_________________________________________________________________\n",
      "flatten_4 (Flatten)          (None, 3968)              0         \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 64)                254016    \n",
      "_________________________________________________________________\n",
      "activation_10 (Activation)   (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "activation_11 (Activation)   (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 1)                 33        \n",
      "_________________________________________________________________\n",
      "activation_12 (Activation)   (None, 1)                 0         \n",
      "=================================================================\n",
      "Total params: 578,225\n",
      "Trainable params: 578,225\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# parameter\n",
    "input_size = 500\n",
    "vocab_size = 10000\n",
    "embedding_size = 32 #word lenth\n",
    "\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, embedding_size, input_length=input_size))\n",
    "model.add(Conv1D(16, kernel_size=2, activation='relu'))\n",
    "model.add(MaxPooling1D())\n",
    "model.add(Conv1D(32, kernel_size=2, activation='relu'))\n",
    "model.add(MaxPooling1D())\n",
    "model.add(Flatten())\n",
    "model.add(Dense(64))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(32))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(1))\n",
    "model.add(Activation('sigmoid'))\n",
    "\n",
    "\n",
    "hist_CV = model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 161788 samples, validate on 40448 samples\n",
      "Epoch 1/10\n",
      "161788/161788 [==============================] - 13s 82us/step - loss: 0.1885 - acc: 0.9360 - val_loss: 0.1384 - val_acc: 0.9507\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.13839, saving model to best_weights.h5\n",
      "Epoch 2/10\n",
      "161788/161788 [==============================] - 13s 78us/step - loss: 0.1254 - acc: 0.9542 - val_loss: 0.1395 - val_acc: 0.9509\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.13839\n",
      "Epoch 3/10\n",
      "161788/161788 [==============================] - 13s 78us/step - loss: 0.1045 - acc: 0.9611 - val_loss: 0.1581 - val_acc: 0.9505\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.13839\n",
      "Epoch 4/10\n",
      "161788/161788 [==============================] - 13s 78us/step - loss: 0.0839 - acc: 0.9684 - val_loss: 0.1659 - val_acc: 0.9423\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.13839\n"
     ]
    }
   ],
   "source": [
    "file_path=\"best_weights.h5\"\n",
    "checkpoint = ModelCheckpoint(file_path, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
    "early = EarlyStopping(monitor=\"val_loss\", mode=\"min\", patience=3)\n",
    "\n",
    "callbacks_list = [checkpoint, early] \n",
    "hist_CV = model.fit(train_data, y_train, validation_data=(test_data, y_test),\n",
    "          batch_size=64, \n",
    "          epochs=10, \n",
    "          shuffle = True,\n",
    "          callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"Hate_calss_CNN.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.97      0.97      0.97     36523\n",
      "         1.0       0.69      0.73      0.71      3925\n",
      "\n",
      "   micro avg       0.94      0.94      0.94     40448\n",
      "   macro avg       0.83      0.85      0.84     40448\n",
      "weighted avg       0.94      0.94      0.94     40448\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "y_true, y_pred = y_test, model.predict_classes(test_data)\n",
    "print(classification_report(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0],\n",
       "       [1]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comment = [\"racism is bad!\", \"go back to your country!\"]\n",
    "comment_sequence = tf.texts_to_sequences(comment)\n",
    "comment_data = pad_sequences(comment_sequence, maxlen=500, padding='post')\n",
    "#print(omment_data.shape)\n",
    "comment_data = np.array(comment_data)\n",
    "model.predict_classes(comment_data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
